{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Sentiment Analysis of IMDB Movie Reviews**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"# **Problem Statement:**\n\n我们将会对IMDB中的影评进行情感分析。学习TFIDF是如何提取文章特征的，使用逻辑回归进行分类","metadata":{}},{"cell_type":"markdown","source":"# **1.Import necessary libraries**","metadata":{"_uuid":"1424638f5259100af9f9a5c1b05bd23cf5b71e51"}},{"cell_type":"code","source":"#Load the libraries\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-11-21T03:17:38.324889Z","iopub.execute_input":"2021-11-21T03:17:38.325191Z","iopub.status.idle":"2021-11-21T03:17:40.824378Z","shell.execute_reply.started":"2021-11-21T03:17:38.325136Z","shell.execute_reply":"2021-11-21T03:17:40.823496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.Import the training dataset**","metadata":{"_uuid":"be1b642cce343f7a8f68f8c91f7c50372cdf4381"}},{"cell_type":"code","source":"#importing the training data\nimdb_data=pd.read_csv('../input/IMDB Dataset.csv')\nprint(imdb_data.shape)\nimdb_data.head(10)","metadata":{"_uuid":"4c593c17588723c0b0b0f19851cb70a8447ced76","scrolled":true,"execution":{"iopub.status.busy":"2021-11-21T03:17:40.826473Z","iopub.execute_input":"2021-11-21T03:17:40.827012Z","iopub.status.idle":"2021-11-21T03:17:42.232942Z","shell.execute_reply.started":"2021-11-21T03:17:40.82696Z","shell.execute_reply":"2021-11-21T03:17:42.23216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3.Sentiment count**","metadata":{"_uuid":"453c3fd238f62ab8f649eb01771817e25bc0c77d"}},{"cell_type":"code","source":"#sentiment count\nimdb_data['sentiment'].value_counts()","metadata":{"_uuid":"cb6bb97b0f851947dcf341a1de5708a1f2bc64c1","execution":{"iopub.status.busy":"2021-11-21T03:17:42.234997Z","iopub.execute_input":"2021-11-21T03:17:42.235531Z","iopub.status.idle":"2021-11-21T03:17:42.254658Z","shell.execute_reply.started":"2021-11-21T03:17:42.235481Z","shell.execute_reply":"2021-11-21T03:17:42.253864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the dataset is balanced.","metadata":{}},{"cell_type":"code","source":"sentiments = imdb_data['sentiment'].apply(lambda x:int(x=='positive'))\nsentiments[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:17:42.256539Z","iopub.execute_input":"2021-11-21T03:17:42.257985Z","iopub.status.idle":"2021-11-21T03:17:42.302979Z","shell.execute_reply.started":"2021-11-21T03:17:42.256809Z","shell.execute_reply":"2021-11-21T03:17:42.302149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.Text normalization**","metadata":{"_uuid":"90da29c3b79f46f41d7391a2a116065b616d0fac"}},{"cell_type":"code","source":"\n#Setting English stopwords\nstopword_list = stopwords.words('english')","metadata":{"_uuid":"f000c43d91f68f6668539f089c6a54c5ce3bd819","execution":{"iopub.status.busy":"2021-11-21T03:17:42.306567Z","iopub.execute_input":"2021-11-21T03:17:42.307093Z","iopub.status.idle":"2021-11-21T03:17:42.31823Z","shell.execute_reply.started":"2021-11-21T03:17:42.306811Z","shell.execute_reply":"2021-11-21T03:17:42.317334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.1 Removing html strips and noise text**","metadata":{"_uuid":"328b6e5977da3e055ad4b2e11a31e5e12ccf3b16"}},{"cell_type":"code","source":"#Removing the html strips\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub(\"[.;:!\\'?,\\\"()\\[\\]]\", ' ', text)\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    return text\n#Apply function on review column\nclean_reviews = imdb_data['review'].apply(denoise_text)","metadata":{"_uuid":"6f6fcafbdadcdcb0c164e37d71fb9d1623f74d0a","execution":{"iopub.status.busy":"2021-11-21T03:17:42.321085Z","iopub.execute_input":"2021-11-21T03:17:42.321504Z","iopub.status.idle":"2021-11-21T03:17:53.855362Z","shell.execute_reply.started":"2021-11-21T03:17:42.321347Z","shell.execute_reply":"2021-11-21T03:17:53.854601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define function for removing special characters and lower\ndef remove_special_characters(text, remove_digits=True):\n    pattern=r'[^a-zA-z\\s]'\n    text=re.sub(pattern,'',text)\n    return text.lower()\n#Apply function on review column\nclean_reviews = clean_reviews.apply(remove_special_characters)","metadata":{"_uuid":"219da72b025121fd98081df50ae0fcaace10cc9d","execution":{"iopub.status.busy":"2021-11-21T03:17:53.856919Z","iopub.execute_input":"2021-11-21T03:17:53.857263Z","iopub.status.idle":"2021-11-21T03:17:55.817586Z","shell.execute_reply.started":"2021-11-21T03:17:53.857213Z","shell.execute_reply":"2021-11-21T03:17:55.814408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 **Text stemming**\n![](https://devopedia.org/images/article/218/8583.1569386710.png)","metadata":{"_uuid":"3b66eeabd5b7b8c251f8b8ddf331140a64bcd514"}},{"cell_type":"code","source":"ps= PorterStemmer()\nprint(ps.stem('consultants'))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:17:55.823663Z","iopub.execute_input":"2021-11-21T03:17:55.824188Z","iopub.status.idle":"2021-11-21T03:17:55.830003Z","shell.execute_reply.started":"2021-11-21T03:17:55.823913Z","shell.execute_reply":"2021-11-21T03:17:55.82907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stemming the text\ndef simple_stemmer(text):\n    ps= PorterStemmer()\n    text= ' '.join([ps.stem(word) for word in text.split()])\n    return text\n#Apply function on review column\nclean_reviews = clean_reviews.apply(simple_stemmer)","metadata":{"_uuid":"2295f2946e0ab74c220ad538d0e7adc04d23f697","execution":{"iopub.status.busy":"2021-11-21T03:17:55.831414Z","iopub.execute_input":"2021-11-21T03:17:55.83198Z","iopub.status.idle":"2021-11-21T03:22:14.770711Z","shell.execute_reply.started":"2021-11-21T03:17:55.831893Z","shell.execute_reply":"2021-11-21T03:22:14.769877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 **Removing stopwords**","metadata":{"_uuid":"e83107e4a281d84d7ae42b4e2c8d81b7ece438e4"}},{"cell_type":"code","source":"#set stopwords to english\nstop=set(stopwords.words('english'))\n\n#removing the stopwords\ndef remove_stopwords(text):\n    tokens = [token.strip() for token in text.split()]\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n#Apply function on review column\nclean_reviews= clean_reviews.apply(remove_stopwords)\nprint(clean_reviews.head())","metadata":{"_uuid":"5dbff82b4d2d188d8777b273a75d8ac714d38885","execution":{"iopub.status.busy":"2021-11-21T03:22:14.772257Z","iopub.execute_input":"2021-11-21T03:22:14.772533Z","iopub.status.idle":"2021-11-21T03:22:38.393785Z","shell.execute_reply.started":"2021-11-21T03:22:14.772489Z","shell.execute_reply":"2021-11-21T03:22:38.39289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5.Spliting the training dataset**","metadata":{"_uuid":"f61964573faababe1f7897b77d32815a24954d2f"}},{"cell_type":"code","source":"#split the dataset  \ntrain_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(clean_reviews, sentiments, test_size=0.2, shuffle=True)\n\nprint(train_reviews.shape,train_sentiments.shape)\nprint(test_reviews.shape,test_sentiments.shape)","metadata":{"_uuid":"d3aaabff555e07feb11c72cc3a6e457615975ffe","execution":{"iopub.status.busy":"2021-11-21T03:22:38.395332Z","iopub.execute_input":"2021-11-21T03:22:38.395792Z","iopub.status.idle":"2021-11-21T03:22:38.412681Z","shell.execute_reply.started":"2021-11-21T03:22:38.395694Z","shell.execute_reply":"2021-11-21T03:22:38.412085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.**Term Frequency-Inverse Document Frequency model (TFIDF)**\n\nTF-IDF 用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度\n\n\nTF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)：\n\n$ tfidf(tiger,doc,D) = tf(tiger,d) \\times idf(t,D) $\n\n* t: Term\n* d: a document\n* D: documents\n\n**Term Frequency**：$tf(t,d) = count(t,d) \\div |d| $  【单词t在文本d中出现的次数 / 文本d中的单词数】\n\n**Inverse Document Frequency**：$idf(t,D) = \\log(|D| \\div |\\{d\\in D : t\\in d\\}| ) $ 【log(D文档数量 / 出现单词t的文档数)】\n\n\n**例子：**\n\n假设有一个文档库D，其中在一个100字的文档doc中， tiger出现了3次，文档库的1000个文档中10个含有tiger则：\n\n\n\n$ tfidf(tiger,doc,D) = (3 \\div 100)  \\times (\\log(1000 \\div 10)) $\n\n\n\n\n","metadata":{"_uuid":"52371868f05ff9cf157280c5acf0f5bc71ee176d"}},{"cell_type":"code","source":"documentA = 'the man went out for a walk'\ndocumentB = 'the children sat around the fire after a walk'\n\nvectorizer = TfidfVectorizer()\nvectorizer.fit([documentA, documentB])   # 字典生成，计算idf\nvectors = vectorizer.transform([documentA, documentB]) # 计算tf和tf-idf\n\n# 展示代码：\nfeature_names = vectorizer.get_feature_names()\ndense = vectors.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:38.41415Z","iopub.execute_input":"2021-11-21T03:22:38.414617Z","iopub.status.idle":"2021-11-21T03:22:38.459499Z","shell.execute_reply.started":"2021-11-21T03:22:38.414537Z","shell.execute_reply":"2021-11-21T03:22:38.458888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tfidf vectorizer\ntv=TfidfVectorizer()\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(train_reviews)\n#transformed test reviews\ntv_test_reviews=tv.transform(test_reviews)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)","metadata":{"_uuid":"afe6de957339921e05a6faeaf731f2272fd31946","execution":{"iopub.status.busy":"2021-11-21T03:22:38.461801Z","iopub.execute_input":"2021-11-21T03:22:38.462242Z","iopub.status.idle":"2021-11-21T03:22:40.60997Z","shell.execute_reply.started":"2021-11-21T03:22:38.462192Z","shell.execute_reply":"2021-11-21T03:22:40.608326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"每个词的tf-idf数值组成一组向量表示文章的特征，在此数据集中每个向量有93301个特征","metadata":{}},{"cell_type":"markdown","source":"# 7.**Modelling the dataset**","metadata":{}},{"cell_type":"markdown","source":"Let us build logistic regression model for both bag of words and tfidf features","metadata":{"_uuid":"d5e45fdc9d062a5b9b9dd665ffe732776e196953"}},{"cell_type":"code","source":"#training the model\nlr=LogisticRegression()\n#Fitting the model for tfidf features\nlr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\nprint(lr_tfidf)","metadata":{"_uuid":"142d007421900550079a12ae8655bcae678ebaad","execution":{"iopub.status.busy":"2021-11-21T03:22:40.611388Z","iopub.status.idle":"2021-11-21T03:22:40.612035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.**Logistic regression model performane on test dataset**","metadata":{"_uuid":"07eb6d52eb32469e3be82e90af636d598a7b7c27"}},{"cell_type":"code","source":"##Predicting the model for tfidf features\nlr_tfidf_predict=lr.predict(tv_test_reviews)\nprint(lr_tfidf_predict)","metadata":{"_uuid":"52ad86935b76117f97b79e6672a3ba12352b9461","execution":{"iopub.status.busy":"2021-11-21T03:22:40.613581Z","iopub.status.idle":"2021-11-21T03:22:40.614297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.1 **Accuracy of the model**","metadata":{}},{"cell_type":"code","source":"#Accuracy score for tfidf features\nlr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.615473Z","iopub.status.idle":"2021-11-21T03:22:40.616114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2**Print the classification report**","metadata":{"_uuid":"ac2ec8353acb5e0f548e1e4a590fbe6f34f4a686"}},{"cell_type":"code","source":"#Classification report for tfidf features\nlr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\nprint(lr_tfidf_report)","metadata":{"_uuid":"f89c7e7a6136d08790ffbf6bc4d0d05455f8555a","execution":{"iopub.status.busy":"2021-11-21T03:22:40.617474Z","iopub.status.idle":"2021-11-21T03:22:40.618128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9.LSTM / RNN","metadata":{}},{"cell_type":"markdown","source":"**Recurrent neural network**\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\" alt=\"drawing\" width=\"200\"/>\n\n**An unrolled recurrent neural network.**\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"drawing\" width=\"600\"/>\n\n**LSTM**\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt=\"drawing\" width=\"600\"/>","metadata":{}},{"cell_type":"code","source":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.619498Z","iopub.status.idle":"2021-11-21T03:22:40.620156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tockenize(x_train,x_val):\n    word_list = []\n\n    for sent in x_train:\n        for word in sent.split():\n            if word != '':\n                word_list.append(word)\n  \n    corpus = Counter(word_list)\n    # sorting on the basis of most common words\n    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n    # creating a dict\n    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n    \n    # tockenize\n    final_list_train,final_list_test = [],[]\n    for sent in x_train:\n            final_list_train.append([onehot_dict[word] for word in sent.split() if word in onehot_dict])\n    for sent in x_val:\n            final_list_test.append([onehot_dict[word] for word in sent.split() if word in onehot_dict])\n            \n\n    return np.array(final_list_train),np.array(final_list_test),onehot_dict","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.622566Z","iopub.status.idle":"2021-11-21T03:22:40.623245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,vocab = tockenize(train_reviews,test_reviews)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.624442Z","iopub.status.idle":"2021-11-21T03:22:40.625122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.array(train_sentiments)\ny_test = np.array(test_sentiments)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.626336Z","iopub.status.idle":"2021-11-21T03:22:40.627029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Length of vocabulary is {len(vocab)}')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.628242Z","iopub.status.idle":"2021-11-21T03:22:40.628888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rev_len = [len(i) for i in x_train]\npd.Series(rev_len).hist()\nplt.show()\npd.Series(rev_len).describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.63011Z","iopub.status.idle":"2021-11-21T03:22:40.630758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## padding","metadata":{}},{"cell_type":"code","source":"def padding_(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.631997Z","iopub.status.idle":"2021-11-21T03:22:40.632666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we have very less number of reviews with length > 500.\n#So we will consideronly those below it.\nx_train_pad = padding_(x_train,500)\nx_test_pad = padding_(x_test,500)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.633919Z","iopub.status.idle":"2021-11-21T03:22:40.634584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batching and loading as tensor","metadata":{}},{"cell_type":"code","source":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n\n# dataloaders\nbatch_size = 64\n\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.635824Z","iopub.status.idle":"2021-11-21T03:22:40.636497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.637722Z","iopub.status.idle":"2021-11-21T03:22:40.638379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n        super(SentimentRNN,self).__init__()\n \n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n \n        self.no_layers = no_layers\n        self.vocab_size = vocab_size\n    \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        #lstm\n        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n                           num_layers=no_layers, batch_first=True)\n        \n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n    \n        # linear and sigmoid layer\n        self.fc = nn.Linear(self.hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        # embeddings and lstm_out\n        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n        #print(embeds.shape)  #[50, 500, 1000]\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n        \n        # dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n        \n        \n        \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        hidden = (h0,c0)\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.639615Z","iopub.status.idle":"2021-11-21T03:22:40.640301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_layers = 2\nvocab_size = len(vocab) + 1 #extra 1 for padding\nembedding_dim = 64\noutput_dim = 1\nhidden_dim = 256\n\n\nmodel = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n\n#moving to gpu\nmodel.to(device)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.641507Z","iopub.status.idle":"2021-11-21T03:22:40.642184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred == label.squeeze()).item()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.64339Z","iopub.status.idle":"2021-11-21T03:22:40.644076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip = 5\nepochs = 5 \nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device), labels.to(device)   \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        \n        model.zero_grad()\n        output,h = model(inputs,h)\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        # calculating accuracy\n        print(output,labels)\n        accuracy = acc(output,labels)\n        train_acc += accuracy\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n \n    \n        \n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader:\n            val_h = tuple([each.data for each in val_h])\n\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            output, val_h = model(inputs, val_h)\n            val_loss = criterion(output.squeeze(), labels.float())\n\n            val_losses.append(val_loss.item())\n            \n            accuracy = acc(output,labels)\n            val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc/len(train_loader.dataset)\n    epoch_val_acc = val_acc/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '../working/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.645326Z","iopub.status.idle":"2021-11-21T03:22:40.645989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n    \nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.64735Z","iopub.status.idle":"2021-11-21T03:22:40.648093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def predict_text(text):\n        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n                         if preprocess_string(word) in vocab.keys()])\n        word_seq = np.expand_dims(word_seq,axis=0)\n        pad =  torch.from_numpy(padding_(word_seq,500))\n        inputs = pad.to(device)\n        batch_size = 1\n        h = model.init_hidden(batch_size)\n        h = tuple([each.data for each in h])\n        output, h = model(inputs, h)\n        return(output.item())","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.649288Z","iopub.status.idle":"2021-11-21T03:22:40.649971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 32\nprint(df['review'][index])\nprint('='*70)\nprint(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\nprint('='*70)\npro = predict_text(df['review'][index])\nstatus = \"positive\" if pro > 0.5 else \"negative\"\npro = (1 - pro) if status == \"negative\" else pro\nprint(f'predicted sentiment is {status} with a probability of {pro}')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T03:22:40.651201Z","iopub.status.idle":"2021-11-21T03:22:40.652007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}